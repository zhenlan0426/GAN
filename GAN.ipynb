{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain = mnist.train.images\n",
    "Xtest = mnist.test.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def xavier_init(inputShape, outputShape): \n",
    "    low = -np.sqrt(6.0/(inputShape + outputShape)) \n",
    "    return tf.random_uniform((inputShape, outputShape), \n",
    "                             minval=low, maxval=-low, \n",
    "                             dtype=tf.float32)\n",
    "\n",
    "def lrelu(x, leak=0.2):\n",
    "    f1 = 0.5 * (1 + leak)\n",
    "    f2 = 0.5 * (1 - leak)\n",
    "    return f1 * x + f2 * tf.abs(x)\n",
    "    \n",
    "class MLP(object):\n",
    "    \n",
    "    def __init__(self,Shapes,actFuns):\n",
    "        self.Shapes = Shapes\n",
    "        self.actFuns = actFuns\n",
    "        self.W = [tf.Variable(xavier_init(Shapes[i],Shapes[i+1])) \n",
    "                                  for i in range(len(Shapes)-1)]\n",
    "        self.b = [tf.Variable(tf.zeros([Shapes[i]])) for i in range(1,len(Shapes))]\n",
    "        self.para = self.W + self.b\n",
    "        \n",
    "    def predict(self,X):\n",
    "        for w,b,fun in zip(self.W, self.b, self.actFuns):\n",
    "            X = fun(tf.matmul(X,w)+b)\n",
    "        return X\n",
    "\n",
    "class multilayerConv(object):\n",
    "    \n",
    "    def __init__(self,Shapes,actFuns,Strides,Padding,forward=True,outputShapes=None):\n",
    "        # Shapes is a list of shape of the form [filter_height, filter_width, in_channels, out_channels]\n",
    "        self.forward = forward # conv or conv_transpose\n",
    "        self.Shapes = Shapes\n",
    "        self.actFuns = actFuns\n",
    "        self.Strides = Strides\n",
    "        self.Padding = Padding \n",
    "        self.outputShapes = outputShapes\n",
    "        self.W = [tf.Variable(tf.random_normal(Shapes[i])/8) for i in range(len(Shapes))]\n",
    "        _index = 3 if forward else 2\n",
    "        self.b = [tf.Variable(tf.zeros([Shapes[i][_index]])) \n",
    "                              for i in range(len(Shapes))]\n",
    "        self.para = self.W + self.b\n",
    "        \n",
    "    def predict(self,X):\n",
    "        if self.forward: \n",
    "            for w,b,fun,stride,padding in zip(self.W, self.b, self.actFuns,self.Strides,self.Padding):\n",
    "                X = fun(tf.nn.conv2d(X,w,stride,padding)+b)\n",
    "        else:\n",
    "            for w,b,output,fun,stride,padding in zip(self.W, self.b, self.outputShapes, self.actFuns,self.Strides,self.Padding):\n",
    "                X = fun(tf.nn.conv2d_transpose(X,w,output,stride,padding)+b)            \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DCGAN(object):\n",
    "    \n",
    "    def __init__(self,G_cnn_Shapes,G_cnn_actFuns,G_cnn_Strides,G_cnn_Padding,G_cnn_outputShapes,\n",
    "                      D_cnn_Shapes,D_cnn_actFuns,D_cnn_Strides,D_cnn_Padding,D_MLP_Shapes,D_MLP_actFuns,\n",
    "                      X_H,X_W,X_D,Z_H,Z_W,Z_D,batchSize,r):\n",
    "        \n",
    "        self.X_shape = [batchSize,X_H,X_W,X_D]\n",
    "        self.X = tf.placeholder(tf.float32,self.X_shape)\n",
    "        self.Z_shape = [batchSize,Z_H,Z_W,Z_D]\n",
    "        self.Z = tf.placeholder(tf.float32,self.Z_shape)\n",
    "        self.r = r\n",
    "        self.batchSize = batchSize\n",
    "        \n",
    "        # Generator\n",
    "        self.G_CNN = multilayerConv(G_cnn_Shapes,G_cnn_actFuns,G_cnn_Strides,G_cnn_Padding,False,G_cnn_outputShapes)\n",
    "        self.X_fake = self.G_CNN.predict(self.Z)\n",
    "        \n",
    "        # Discriminator\n",
    "        self.D_CNN = multilayerConv(D_cnn_Shapes,D_cnn_actFuns,D_cnn_Strides,D_cnn_Padding)\n",
    "        self.D_MLP = MLP(D_MLP_Shapes,D_MLP_actFuns)\n",
    "        D_predict = lambda x: self.D_MLP.predict(self.D_CNN.predict(x))\n",
    "        D_fake = D_predict(self.X_fake)\n",
    "        D_real = D_predict(self.X)\n",
    "        \n",
    "        # loss\n",
    "        self.G_loss = -tf.reduce_mean(tf.log(D_fake))\n",
    "        self.D_loss = -tf.reduce_mean(tf.log(D_real)+tf.log(1-D_fake))\n",
    "        #loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real, \n",
    "        #                                                                   labels=tf.ones_like(D_real)*0.9))\n",
    "        #loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake,\n",
    "        #                                                                   labels=tf.zeros_like(D_fake)))\n",
    "        #self.D_loss = loss_real + loss_fake\n",
    "        #self.G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake, labels=tf.ones_like(D_fake)))\n",
    "\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer_G = tf.train.AdamOptimizer(learning_rate=self.r).minimize(self.G_loss,var_list=self.G_CNN.para)\n",
    "        self.optimizer_D = tf.train.AdamOptimizer(learning_rate=self.r).minimize(self.D_loss,var_list=self.D_CNN.para+self.D_MLP.para)\n",
    "        \n",
    "        # session\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(init)\n",
    "    \n",
    "    def _partial_fit(self,X_np):\n",
    "        #_,_,loss_G,loss_D = self.sess.run([self.optimizer_G,self.optimizer_D,self.G_loss,self.D_loss],\n",
    "        #                          {self.X:X_np, self.Z:np.random.randn(*self.Z_shape)})\n",
    "        _,loss_G = self.sess.run([self.optimizer_G,self.G_loss],\n",
    "                                          {self.Z:np.random.randn(*self.Z_shape)})\n",
    "        _,loss_D = self.sess.run([self.optimizer_D,self.D_loss],\n",
    "                                          {self.X:X_np, self.Z:np.random.randn(*self.Z_shape)})        \n",
    "        return loss_G,loss_D\n",
    "    \n",
    "    def fit(self,X,iterations):\n",
    "        N = X.shape[0]\n",
    "        n = N/self.batchSize\n",
    "        for i in range(iterations):\n",
    "            index_ = np.random.permutation(N)\n",
    "            cumLoss_G,cumLoss_D = 0,0\n",
    "            for j in range(n):\n",
    "                Loss_G,Loss_D = self._partial_fit(X[index_[j*self.batchSize:(j+1)*self.batchSize]])\n",
    "                cumLoss_G += Loss_G\n",
    "                cumLoss_D += Loss_D\n",
    "            print \"iter: {}, loss_G: {}, loss_D: {}\".format(i,cumLoss_G/n, cumLoss_D/n)        \n",
    "    \n",
    "    def sample(self):\n",
    "        return self.sess.run(self.X_fake,{self.Z:np.random.randn(*self.Z_shape)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GAN(object):\n",
    "    \n",
    "    def __init__(self,GShapes,DShapes,zShape,batchSize,GActFun,DActFun,r):\n",
    "        self.X = tf.placeholder(tf.float32,[batchSize,GShapes[-1]])\n",
    "        self.Z = tf.placeholder(tf.float32,[batchSize,zShape])\n",
    "        self.zShape = zShape\n",
    "        self.r = r\n",
    "        self.batchSize = batchSize\n",
    "        \n",
    "        # Generator\n",
    "        self.G_MLP = MLP(GShapes,GActFun)\n",
    "        self.X_fake = self.G_MLP.predict(self.Z)\n",
    "        \n",
    "        # Discriminator\n",
    "        self.D_MLP = MLP(DShapes,DActFun)\n",
    "        D_fake = self.D_MLP.predict(self.X_fake)\n",
    "        D_real = self.D_MLP.predict(self.X)\n",
    "        \n",
    "        # loss\n",
    "        self.G_loss = -tf.reduce_mean(tf.log(D_fake))\n",
    "        self.D_loss = -tf.reduce_mean(tf.log(D_real)+tf.log(1-D_fake))\n",
    "        #loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real, \n",
    "        #                                                                   labels=tf.ones_like(D_real)*0.9))\n",
    "        #loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake,\n",
    "        #                                                                   labels=tf.zeros_like(D_fake)))\n",
    "        #self.D_loss = loss_real + loss_fake\n",
    "        #self.G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake, labels=tf.ones_like(D_fake)))\n",
    "\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer_G = tf.train.AdamOptimizer(learning_rate=self.r).minimize(self.G_loss,var_list=self.G_MLP.para)\n",
    "        self.optimizer_D = tf.train.AdamOptimizer(learning_rate=self.r).minimize(self.D_loss,var_list=self.D_MLP.para)\n",
    "        \n",
    "        # session\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(init)\n",
    "    \n",
    "    def _partial_fit(self,X_np):\n",
    "        #_,_,loss_G,loss_D = self.sess.run([self.optimizer_G,self.optimizer_D,self.G_loss,self.D_loss],\n",
    "        #                          {self.X:X_np, self.Z:Z_np})\n",
    "        _,loss_G = self.sess.run([self.optimizer_G,self.G_loss],\n",
    "                                          {self.Z:np.random.randn(self.batchSize,self.zShape)})\n",
    "        _,loss_D = self.sess.run([self.optimizer_D,self.D_loss],\n",
    "                                          {self.X:X_np, self.Z:np.random.randn(self.batchSize,self.zShape)})        \n",
    "        return loss_G,loss_D\n",
    "    \n",
    "    def fit(self,X,iterations):\n",
    "        N = X.shape[0]\n",
    "        n = N/self.batchSize\n",
    "        for i in range(iterations):\n",
    "            index_ = np.random.permutation(N)\n",
    "            cumLoss_G,cumLoss_D = 0,0\n",
    "            for j in range(n):\n",
    "                Loss_G,Loss_D = self._partial_fit(X[index_[j*self.batchSize:(j+1)*self.batchSize]])\n",
    "                cumLoss_G += Loss_G\n",
    "                cumLoss_D += Loss_D\n",
    "            print \"iter: {}, loss_G: {}, loss_D: {}\".format(i,cumLoss_G/n, cumLoss_D/n)        \n",
    "    \n",
    "    def sample(self):\n",
    "        return self.sess.run(self.X_fake,{self.Z:np.random.randn(self.batchSize,self.zShape)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gan = GAN([100,400,784],[784,400,1],100,100,[lrelu,tf.nn.sigmoid],[lrelu,tf.nn.sigmoid],1e-4)\n",
    "#gan = GAN([100,400,784],[784,400,1],100,100,[tf.nn.relu,tf.nn.sigmoid],[tf.nn.relu,tf.nn.sigmoid],1e-3)\n",
    "#gan = GAN([100,400,784],[784,400,1],100,100,[lrelu,tf.nn.sigmoid],[lrelu,tf.nn.sigmoid],1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gan.fit(Xtrain,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_sample = gan.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for s in X_sample:\n",
    "    plt.imshow(np.reshape(s,(28,28)),cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchSize = 20\n",
    "r = 1e-6\n",
    "G_cnn_Shapes = [(5,5,32,2),(5,5,64,32),(5,5,1,64)] # [height, width, output_channels, in_channels]\n",
    "G_cnn_actFuns = [lrelu,lrelu,tf.nn.sigmoid]\n",
    "G_cnn_Strides = [(1,2,2,1),(1,2,2,1),(1,1,1,1)]\n",
    "G_cnn_Padding = ['SAME']*3\n",
    "G_cnn_outputShapes = [(batchSize,14,14,32),(batchSize,28,28,64),(batchSize,28,28,1)] # [batch, height, width, channels]\n",
    "\n",
    "D_cnn_Shapes = [(3,3,1,32),(3,3,32,64),(3,3,64,128)] # [filter_height, filter_width, in_channels, out_channels]\n",
    "D_cnn_actFuns = [lrelu,lrelu,lambda x:tf.reduce_mean(lrelu(x),(1,2))]\n",
    "D_cnn_Strides = [(1,2,2,1),(1,2,2,1),(1,1,1,1)]\n",
    "D_cnn_Padding = ['SAME']*3\n",
    "D_MLP_Shapes = [(128,64,1)]\n",
    "D_MLP_actFuns = [lrelu, tf.nn.sigmoid]\n",
    "X_H,X_W,X_D = 28,28,1\n",
    "Z_H,Z_W,Z_D = 7,7,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dcgan = DCGAN(G_cnn_Shapes,G_cnn_actFuns,G_cnn_Strides,G_cnn_Padding,G_cnn_outputShapes,\n",
    "                      D_cnn_Shapes,D_cnn_actFuns,D_cnn_Strides,D_cnn_Padding,D_MLP_Shapes,D_MLP_actFuns,\n",
    "                      X_H,X_W,X_D,Z_H,Z_W,Z_D,batchSize,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dcgan.fit(Xtrain.reshape([-1,28,28,1]),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def compose(*functions):\n",
    "    def compose2(f, g):\n",
    "        return lambda x: f(g(x))\n",
    "    return functools.reduce(compose2, functions, lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
